{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38b768ec-810b-4b7c-9aaf-3ac91f24ce38",
   "metadata": {},
   "source": [
    "# Lead role preprocessing\n",
    "\n",
    "A very important piece of information to validate our hypothesis, but one that is very difficult to obtain, is the importance of the actors' roles within each movie. To get reliable information on this, we looked at the free public data available and found the script data for the most directly relevant movies. Our goal is to compute how much of a role each character plays, given the movie plot data we have and the names of the characters in each movie. As a first step, we prepared a script that would be our gold label, from which we calculated the percentage of each character in the movie. We used this script and mapped it to the plot to train and validate our AI model.\n",
    "\n",
    "## 1. Script crawling\n",
    "\n",
    "To crawl the scripts, we selected https://imsdb.com and wrote a crawler for it. As the site is very old, the templates between the script documents are very different, and the crawling difficulty is high.\n",
    "\n",
    "### A. Downloading the scripts\n",
    " \n",
    "```bash\n",
    "python Preprocessing/script_crawling/get_script_urls.py\n",
    "```\n",
    "Running the script above will produce scripts_urls.json, which contains meta information of urls we are going to crawl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "147e901a-dd1a-44f1-b4d1-29c2320dbef0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from urllib.parse import quote\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "root_path = Path('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "2678d516-3ba5-4b62-955c-0d1d0b956a03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of script: 1187\n",
      "Scripts with release date field : 641 \n",
      "\twith script date field: 677 \n"
     ]
    }
   ],
   "source": [
    "scripts_urls= json.load(open('scripts_urls.json'))\n",
    "print(f\"Total number of script: {len(scripts_urls)}\")\n",
    "print(f\"Scripts with release date field : {len([ x for x in scripts_urls if 'release_date' in x])} \")\n",
    "print(f\"\\twith script date field: {len([ x for x in scripts_urls if 'script_date' in x])} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1370ea-fd2d-4025-9b07-890ec07432c9",
   "metadata": {},
   "source": [
    "Since our movie data doens't have any movie identifier, it's release date is very important feature to map movie with tmdb data with high accuracy.\n",
    "\n",
    "Running **download_scripts.py** will produce url2pairs.json, which has beautifulsoup object that contain complex web component including our target scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4011fbbb-5f81-45ae-8f31-5dee402b5cab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url2pairs = json.load(open('url2pairs.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c2e3d0-559d-44b3-8fca-cd080cdff4d3",
   "metadata": {},
   "source": [
    "# Processing scripts\n",
    "## step 1. Mapping TMDB Movie id by using movie name, year\n",
    "\n",
    "```bash\n",
    "python Preprocessing/script_crawling/scripts_tmdb_matching.py\n",
    "```\n",
    "Run above code will generate mapping of movies to TMDB, *scripts_urls.json*. TMDB search API will retuen several movies when we send the movie name as query. Among the results, we eliminated all but one sample that exactly fulfills the year condition. If released_year exists, use it as top prioroty. If not, pick closer to script_date, but after script_date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8dcd3baa-3711-4dc3-9abd-a35bcf91b0a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "manual_matching = json.load(open('manual_matching.json')) # handcrafted feature. Was so painful.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "085f409f-6cf2-4a1d-b597-25a5b15e2606",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scripts_urls = json.load(open('tmdb_matched_scripts_urls.json','w')) # total 1060 sample matched."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3250bedc-95b5-4459-8431-40bfc3569162",
   "metadata": {
    "tags": []
   },
   "source": [
    "## step 2. matching movie plots.\n",
    "we have three major sources for the movie plots.\n",
    "1. Our original CMU plot data. https://www.cs.cmu.edu/~ark/personas/\n",
    "2. MPST movie plot. https://www.kaggle.com/datasets/cryptexcode/mpst-movie-plot-synopses-with-tags\n",
    "3. Wikipedia movie plot. https://www.kaggle.com/datasets/jrobischon/wikipedia-movie-plots\n",
    "\n",
    "CMU data has freedase id, MPST data has imdb id, and wikipedia plot has only the name. So the priority is MPST > CMU > Wikipedia plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf824f8f-8ca1-4e30-bf7e-e30466b97c2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scripts_urls = json.load(open('tmdb_matched_scripts_urls.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "001e3c0d-d7b1-43c5-81a0-be8215480a8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmu = pd.read_csv('../Data/MovieSummaries/plot_summaries.txt',delimiter='\\t',header=None)\n",
    "mpst = pd.read_csv('../Data/MovieSummaries/mpst_full_data.csv')\n",
    "wiki = pd.read_csv('../Data/MovieSummaries/wiki_movie_plots_deduped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48dc1011-b939-4335-9df3-86a253ed4783",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wiki_id2tmdb_id = json.load(open('../Data/tmdb_resources/wikipedia_id2tmdb_id.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be98a938-984f-4c4b-afc5-9c0d86972784",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmu2tmdb = json.load(open('../Data/tmdb_resources/cmu_exist_tmdb_id2detail.json'))\n",
    "tmdb_id2detail = json.load(open('../Data/tmdb_resources/tmdb_id2detail_imdb_rating.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8319d66-0c9d-42a4-9eba-a3566c94006d",
   "metadata": {},
   "source": [
    "**Now let's prepare tmdb_id2plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e05599be-0b19-49c4-9945-a8a1a8b60289",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_tmdb_ids = [m['tmdb_id'] for m in scripts_urls if 'tmdb_id' in m]\n",
    "tmdb_id2plot = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "a0c7cdbf-0401-4417-874f-4bc68078bb37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid matching by mpst: 942\n"
     ]
    }
   ],
   "source": [
    "# 1. mpst\n",
    "imdb_id2plot = {x['imdb_id']:x['plot_synopsis'] for _,x in mpst.iterrows()}\n",
    "cnt = 0\n",
    "\n",
    "for tmdb_id in tmdb_id2detail.keys():\n",
    "    imdb_id = tmdb_id2detail[str(tmdb_id)]['imdb_id']\n",
    "    if imdb_id in imdb_id2plot:\n",
    "        tmdb_id2plot[tmdb_id] = imdb_id2plot[imdb_id]\n",
    "        cnt += 1\n",
    "print(f\"valid matching by mpst: {cnt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5afc0bfe-be15-4272-a745-7acc26469cf8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid matching by cmu: 32280\n",
      "union to mpst is : 32280\n"
     ]
    }
   ],
   "source": [
    "# 2. cmu\n",
    "wiki_id2plot = {x[0]:x[1] for _,x in cmu.iterrows()}\n",
    "tmdb_id2wiki_id = {v:k for k,v in wiki_id2tmdb_id.items()}\n",
    "wiki_id2tmdb_id\n",
    "cnt = 0\n",
    "for tmdb_id, wiki_id in tmdb_id2wiki_id.items():\n",
    "    if int(wiki_id) in wiki_id2plot:\n",
    "        cnt += 1\n",
    "        if tmdb_id in tmdb_id2plot:\n",
    "            if len(tmdb_id2plot[tmdb_id]) > len(wiki_id2plot[int(wiki_id)]): # to keep shorter plot\n",
    "                tmdb_id2plot[tmdb_id] = wiki_id2plot[int(wiki_id)]\n",
    "        else:\n",
    "            tmdb_id2plot[tmdb_id] = wiki_id2plot[int(wiki_id)]\n",
    "print(f\"valid matching by cmu: {cnt}\")\n",
    "print(f\"union to mpst is : {len(list(tmdb_id2plot.keys()))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "b17de194-6934-40ed-92e0-f7eac8baf66d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "additional backups from wiki data : 30\n",
      "We have total : 1041\n"
     ]
    }
   ],
   "source": [
    "# 3. wiki\n",
    "remain_ids = [tmdb_ids for tmdb_ids in tmdb_id2detail.keys() if tmdb_ids not in tmdb_id2plot]\n",
    "cnt = 0\n",
    "title2plots = {x['Title'].lower():x['Plot'] for _,x in wiki.iterrows()}\n",
    "for tmdb_id in remain_ids:\n",
    "    movie_title = tmdb_id2detail[str(tmdb_id)]['original_title']\n",
    "    if movie_title.lower() in title2plots:\n",
    "        tmdb_id2plot[tmdb_id] = title2plots[movie_title.lower()]\n",
    "        cnt += 1\n",
    "\n",
    "print(f\"additional backups from wiki data : {cnt}\")\n",
    "print(f\"We have total : {len(list(tmdb_id2plot.keys()))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ba3e476-3060-4127-babe-456034923ceb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# json.dump(tmdb_id2plot,open('tmdb_id2plot_cmu_only.json','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0fbe58-572a-4b44-9e35-4420a2aa733b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Step 3. Parsing character's script\n",
    "Strategy : map script's speaker to tmdb character list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "901fe0b2-04fd-4413-ba5d-9b2583c1d075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmdb_id2plot = json.load(open('tmdb_id2plot.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "36112dfd-5a23-4378-868c-78079671995f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmdb_id2credit = json.load(open('../Data/tmdb_resources/tmdb_id2credit_full.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d06d75-b770-48f9-a33e-e1ac88aa1a2a",
   "metadata": {},
   "source": [
    "#### Classification rule\n",
    "**Count the number of space or '\\t' in front of the line.**\n",
    "\n",
    "Assumption : **same type of instruction will share same indent in a single script.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4149d873-7e99-4c27-95f4-5b11106dfcad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def match_characters(A,B):\n",
    "    '''\n",
    "    for A,B = Set[str], return one-one matching dictionary {a:b} s.t a in A, b in B\n",
    "    Only consider strict subset and one-one matching.\n",
    "    If more than one correspondence is possible, omit that name.\n",
    "    '''\n",
    "    mapper = {}\n",
    "    for a in A:\n",
    "        for b in B:\n",
    "            if a.lower() in b.lower() or b.lower() in a.lower():\n",
    "                if a in mapper:\n",
    "                    del mapper[a]\n",
    "                    break\n",
    "                b_overlap = False\n",
    "                del_k_list=[]\n",
    "                for k,v in mapper.items():\n",
    "                    if v == b:\n",
    "                        del_k_list.append(k)\n",
    "                        b_overlap = True\n",
    "                for k in del_k_list:\n",
    "                    del mapper[k]\n",
    "                if b_overlap:\n",
    "                    break\n",
    "                mapper[a] = b\n",
    "    return mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988bedda-9299-47d6-8c0d-325643375da3",
   "metadata": {},
   "source": [
    "Final schema of matched scripts\n",
    "```json\n",
    "tmdb_id2matched_scripts = {\n",
    "    tmdb_id: {\n",
    "        'scripts': [{\n",
    "            'tmdb_credit': tmdb_credit_obj,\n",
    "            'portion': float,\n",
    "            'spoken_syllables': int,\n",
    "            'num_script': int,\n",
    "            'script_text': {\n",
    "                1: \"1st script. order starts from 1.\",\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        'statistics': {\n",
    "            'total_spoken_syllables': int,\n",
    "            'total_spoken_words': int,\n",
    "            'original_character_count': int,\n",
    "            'matched_character_count': int,\n",
    "        },\n",
    "        'url': str\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1ae47d57-5973-4f13-8d7f-a77020ea986e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyphen\n",
    "\n",
    "def get_syllables(text):\n",
    "    dic = pyphen.Pyphen(lang='en')\n",
    "    return sum(len(dic.inserted(word).split('-')) for word in text.split())\n",
    "\n",
    "def count_indent(text):\n",
    "    count = 0\n",
    "    indent_type = ''\n",
    "    \n",
    "    for char in text:\n",
    "        if char == ' ' or char == '\\t':\n",
    "            count += 1\n",
    "            indent_type = 's' if char == ' ' else 't'\n",
    "\n",
    "        else:\n",
    "            break  # Stop counting when a non-indent character is encountered\n",
    "\n",
    "    return f'{count}{indent_type}'\n",
    "\n",
    "def remove_tags(text):\n",
    "    return re.sub(r'<[^>]*>', '', text)\n",
    "def remove_parenthesis(text):\n",
    "    return re.sub(r'\\([^)]*\\)', '', text)\n",
    "def reduce_adjacent_spaces(text):\n",
    "    return re.sub(r'\\s{2,}', ' ', text.replace('\\t',' '))\n",
    "def final_removal_special_characters(text):\n",
    "    return re.sub(r'[,\\[\\]{}:;,.\\^()]', '', text).strip()\n",
    "\n",
    "url2typed_pairs = {} # typed_pairs : {'4s': [(speaker,text),(speaker,text),(speaker,text)...],'3s':[(speaker,text),...]}\n",
    "for url, pairs in url2pairs.items():\n",
    "    url2typed_pairs[url] = defaultdict(list)\n",
    "    for pair in pairs:\n",
    "        for k,v in pair.items():\n",
    "            indent_code = count_indent(k)\n",
    "            new_key = final_removal_special_characters(remove_parenthesis(remove_tags(k.replace('\\t',''))))\n",
    "            new_val = reduce_adjacent_spaces(remove_parenthesis(remove_tags(v))).strip()\n",
    "            if new_key and new_val:      \n",
    "                url2typed_pairs[url][indent_code].append((new_key,new_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "03b4956e-b687-47e5-9887-8cd2b363a8ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bee0cb4b5e742eba57e62a661e1218b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmdb_id2matched_scripts = {} # url2matched_scripts[url] = {}\n",
    "for movie_meta in tqdm(scripts_urls):\n",
    "    if 'tmdb_id' not in movie_meta:\n",
    "        continue\n",
    "    if movie_meta['script_url'] not in url2typed_pairs:\n",
    "        continue\n",
    "    tmdb_id = movie_meta['tmdb_id']\n",
    "    credit = tmdb_id2credit[str(tmdb_id)]\n",
    "    character_list = [c['character'] for c in credit['cast']]\n",
    "    character_dict = url2typed_pairs[movie_meta['script_url']]\n",
    "    max_match = -1\n",
    "    max_indent_type = ''\n",
    "    max_mapper = {}\n",
    "    for c,v in character_dict.items():\n",
    "        instruction_keys = set([x[0] for x in v]) # considering typo, several characters can be matched to one actor.\n",
    "        character_mapper = match_characters(instruction_keys,character_list)\n",
    "        if len(list(character_mapper.keys())) > max_match:\n",
    "            max_match = len(list(character_mapper.keys()))\n",
    "            max_indent_type = c\n",
    "            max_mapper = character_mapper\n",
    "    if max_match > 2:\n",
    "        instruction_keys = set([x[0] for x in character_dict[max_indent_type]])\n",
    "        syll_cnt = { k: sum([get_syllables(t[1]) for t in character_dict[max_indent_type] if t[0] == k ]) for k in instruction_keys}\n",
    "        total_syllables = sum(list(syll_cnt.values()))\n",
    "        char_scripts = defaultdict(dict)\n",
    "        idx = 1\n",
    "        for char_name,text in character_dict[max_indent_type]:\n",
    "            char_scripts[char_name][idx] = text\n",
    "            idx+=1\n",
    "        tmdb_id2matched_scripts[tmdb_id] = {\n",
    "            'scripts': [\n",
    "                {\n",
    "                'tmdb_credit': [_ for _ in credit['cast'] if _['character'] == v][0],\n",
    "                'portion': syll_cnt[k] / total_syllables,\n",
    "                'spoken_syllables': syll_cnt[k],\n",
    "                'num_script': len(list(char_scripts.keys())),\n",
    "                'name_in_script': v,\n",
    "                'script_text': char_scripts[k],\n",
    "                }\n",
    "                for k,v in max_mapper.items()\n",
    "            ],\n",
    "            'statistics':{\n",
    "                'total_spoken_syllables': total_syllables,\n",
    "                'total_spoken_words': sum([sum([len(t[1].split(' ')) for t in character_dict[max_indent_type] if t[0] == k ]) for k in max_mapper.keys()]),\n",
    "                'original_character_count': len(credit['cast']),\n",
    "                'matched_character_count': len(list(max_mapper.keys())),\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ff8231dd-ed2a-4d21-b774-0afa6bcf2a96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json.dump(tmdb_id2matched_scripts,open('tmdb_id2matched_scripts.json','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792572e9-8122-41b0-904a-9ee3650e8eb2",
   "metadata": {},
   "source": [
    "## Step 4. Calculating each character's share of the overall script\n",
    "Estimate speaking time by the number of syllables in each sentense.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67403647-2ad9-4f09-a5dc-966181601f6b",
   "metadata": {},
   "source": [
    "# Training model\n",
    "## 1. Training Setting (Problem Definition)\n",
    "Input: f\"Estimate the portion of {character} in float form from this plot:{plot}\"\n",
    "\n",
    "Output: float(0~1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b344376-f827-4c79-933f-5fb42b36e2d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "tmdb_id2matched_scripts = json.load(open('tmdb_id2matched_scripts.json'))\n",
    "tmdb_id2plot = json.load(open('tmdb_id2plot.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64931b7b-cf9b-4e8b-a308-b7851b52458c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total script num: 942\n",
      "Total actors num: 13760\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total script num: {len(tmdb_id2matched_scripts.keys())}\")\n",
    "print(f\"Total actors num: {sum([len(x['scripts']) for x in tmdb_id2matched_scripts.values()])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd63b697-abfb-42d1-adfb-2b4246c74a5f",
   "metadata": {},
   "source": [
    "split train : eval : test = 8:1:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2742451c-93a6-4cd0-a1d2-eccd77ee92cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set: 709\n",
      "\tCharacter count 11668\n",
      "Evaluation Set: 94\n",
      "\tCharacter count 792\n",
      "Test Set: 94\n",
      "\tCharacter count 791\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "random.seed(1)\n",
    "tmdb_ids =  list(tmdb_id2matched_scripts.keys())\n",
    "tmdb_ids = sorted(tmdb_ids, key=lambda x: len(tmdb_id2matched_scripts[x]['scripts']) + random.randint(-5,5))\n",
    "num_docs = len(tmdb_id2matched_scripts.keys())\n",
    "ratios = {'train': 0.8, 'evaluation': 0.1, 'test': 0.1}\n",
    "\n",
    "set_sizes = {set_name: int(ratio * num_docs) for set_name, ratio in ratios.items()}\n",
    "\n",
    "sets = defaultdict(list)\n",
    "scripts_in_sets = defaultdict(list)\n",
    "\n",
    "# Sequentially add script into each set\n",
    "for tmdb_id in tmdb_ids:\n",
    "    script_obj = tmdb_id2matched_scripts[tmdb_id]\n",
    "    selected_set = min(set_sizes.keys(), key=lambda x: len(scripts_in_sets[x]))\n",
    "    if tmdb_id not in tmdb_id2plot:\n",
    "        continue\n",
    "    sets[selected_set].append(tmdb_id)\n",
    "    all_c = [c['tmdb_credit']['character'] for c in script_obj['scripts']]\n",
    "    scripts_in_sets[selected_set] += [{\n",
    "        'plot': tmdb_id2plot[tmdb_id],\n",
    "        'character': c['tmdb_credit']['character'],\n",
    "        'all_characters': all_c,\n",
    "        'portion': c['portion'] * 100,\n",
    "        'tmdb_id': tmdb_id,\n",
    "        'character_id': c['tmdb_credit']['id'],\n",
    "        'order': c['tmdb_credit']['order'],\n",
    "    } for c in script_obj['scripts']]\n",
    "\n",
    "    set_sizes[selected_set] -= 1\n",
    "    if set_sizes[selected_set] == 0:\n",
    "        del set_sizes[selected_set]\n",
    "    \n",
    "\n",
    "for set_name, docs in sets.items():\n",
    "    print(f\"{set_name.title()} Set: {len(docs)}\")\n",
    "    print(f\"\\tCharacter count {len(scripts_in_sets[set_name])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc936313-1dac-4281-98ee-e6cbb72ca13d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json.dump(scripts_in_sets, open('plot_portion_dataset.json','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592a23d2-a810-4f88-9602-e41621de1c16",
   "metadata": {},
   "source": [
    "Now, prepare dataset for machine learning.\n",
    "\n",
    "Prompt: \"\"Predict the percentage of a movie's plot that a character takes up.\\nCharacter: {} \\nPlot: \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "131ed976-d88a-496d-9d35-4259f811de43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "scripts_in_sets = json.load(open('plot_portion_dataset.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a898a203-f006-4238-a006-f0e5f1039224",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "train_df = pd.DataFrame(scripts_in_sets['train'])\n",
    "train_dataset = Dataset.from_pandas(train_df).shuffle(seed=7)\n",
    "evaluation_df = pd.DataFrame(scripts_in_sets['evaluation'])\n",
    "evaluation_dataset = Dataset.from_pandas(evaluation_df).shuffle(seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43ff3acc-4ba3-42e5-b39b-9ae7853dd529",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def get_prompt(character_name, plot):\n",
    "    c_name = re.sub(r'\\([^)]*\\)', '', character_name).strip()\n",
    "    return f\"Predict the percentage of a movie's plot that a character takes up.\\nCharacter: {c_name} \\nPlot: {plot}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5aa893d8-0b29-4aea-88dc-5190396f267a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'plot': \"NASA Space Shuttle Explorer, commanded by veteran astronaut Matt Kowalski, is in Earth orbit on mission STS-157 to service the Hubble Space Telescope. Dr. Ryan Stone is aboard on her first space mission as a mission specialist, her job being to perform a set of hardware upgrades on the Hubble. During a spacewalk, Mission Control in Houston warns Explorer's crew about a Russian missile strike on a defunct satellite, which has inadvertently caused a chain reaction forming a rapidly-expanding cloud of space debris, ordering the crew to return to Earth immediately. Communication with Mission Control is lost shortly thereafter as more and more communication satellites are knocked out by the debris.\\r\\nHigh-speed debris strikes the Explorer and Hubble, tearing Stone from the shuttle and leaving her tumbling through space. Kowalski, using a Manned Maneuvering Unit (MMU), rescues Stone, and they return to the Explorer, soon discovering that the Shuttle has suffered catastrophic damage and the rest of the crew are dead. Stone and Kowalski decide to use the MMU to reach the International Space Station, which is in orbit about 1,450\\xa0km (900\\xa0mi) away, Kowalski estimating that they have 90 minutes before the debris field completes an orbit and threatens them again.\\r\\nOn their way to the International Space Station (ISS), the two discuss Stone's home life and her daughter, who died young in an accident. As they approach the station, they see that the ISS's crew has evacuated into one of its two Soyuz spacecraft, the remaining Soyuz's parachute being prematurely deployed, rendering it unable to return to Earth. Kowalski suggests using it to travel to the nearby Chinese space station Tiangong, 100\\xa0km (60\\xa0mi) away, in order to board its docked Shenzhou spacecraft and return safely to Earth in its re-entry capsule. Out of air and maneuvering fuel, the two try to grab onto the ISS; Stone's leg gets entangled in the Soyuz's parachute cords and she grabs a strap on Kowalski's suit, but it soon becomes clear that the cords will not support them both. Despite Stone's protests, Kowalski detaches himself from the tether to save her from drifting away with him. Stone is pulled back towards the ISS, while Kowalski floats away.\\r\\nStone enters the space station via the airlock of the Pirs Docking Compartment. She cannot re-establish communication with Kowalski nor Earth and concludes that she is now the sole survivor. Inside the station, a fire breaks out, forcing her to rush to the Soyuz. As she maneuvers the Soyuz away from the ISS, the tangled parachute tethers snag, preventing the spacecraft from leaving; Stone performs a spacewalk to cut the cables, succeeding just as the debris field returns, destroying the station. Stone angles the Soyuz towards Tiangong, but soon discovers that the Soyuz's engine has no fuel.\\r\\nAfter a poignant attempt at radio communication with an Eskimo on Earth, Stone resigns herself to her fate and shuts down the cabin's oxygen supply to commit suicide. As she begins to lose consciousness, Kowalski enters the capsule; scolding her for giving up, he tells her to rig the Soyuz's soft landing rockets to propel the capsule toward Tiangong before disappearing. Realizing Kowalski's appearance was a hallucination, Stone regains the will to go on, restoring the spacecraft's oxygen flow and rigging the landing rockets to propel the capsule towards Tiangong.\\r\\nUnable to dock with Tiangong, Stone ejects herself from the Soyuz and uses a fire extinguisher as a makeshift thruster to travel to the rapidly deorbiting Tiangong. Stone manages to enter Tiangong's Shenzhou spacecraft just as the station enters the upper atmosphere, unlocking its re-entry capsule just in time.\\r\\nThe Shenzhou capsule re-enters the atmosphere successfully; however, during the descent it is damaged by debris from the disintegrating Tiangong and a fire starts inside the capsule. The capsule lands in a lake, but dense smoke forces Stone to evacuate immediately after splashdown, shedding her spacesuit and swimming ashore. Stone shakily takes her first steps back on land.\",\n",
       " 'character': 'Explorer Captain (voice)',\n",
       " 'all_characters': ['Explorer Captain (voice)',\n",
       "  'Mission Control (voice)',\n",
       "  'Shariff (voice)',\n",
       "  'Dr. Ryan Stone'],\n",
       " 'portion': 0.18877216021011162,\n",
       " 'tmdb_id': '49047',\n",
       " 'character_id': 1223442,\n",
       " 'order': 5}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scripts_in_sets['evaluation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9546fd1d-deb7-4f48-87d3-79378ec0838f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a63b11e-2748-4a34-b577-949c2ac50c2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/doni/anaconda3/envs/epfl/lib/python3.11/site-packages/transformers/models/t5/tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "696d9c5ae4ef433796c788302e0ac10f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/doni/anaconda3/envs/epfl/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3856: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "126cf597e93d40fe928de5b2353855da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/792 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 't5-large'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    inputs = [get_prompt(c_name,plot) for c_name,plot in zip(examples['character'],examples['plot'])]\n",
    "    model_inputs = tokenizer(inputs, max_length=1536, truncation=True, padding='max_length')\n",
    "\n",
    "    # Tokenize the targets with padding\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer([str(round(p,2)) for p in examples['portion']], max_length=128, truncation=True, padding='max_length')\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "dataset = train_dataset\n",
    "tokenized_train_dataset = train_dataset.map(preprocess_data, batched=True)\n",
    "tokenized_validation_dataset = evaluation_dataset.map(preprocess_data, batched=True)\n",
    "# tokenized_test_dataset = test_dataset.map(preprocess_data, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d17c84e4-8aa3-43c2-a013-cf9c46893d78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c879e016-0e16-4e61-b046-d1afda9c5441",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11668' max='11668' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11668/11668 12:51:45, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.049300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.048300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.049200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.048900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.050100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.049500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.049200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.048100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.048200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.048600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.049900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f'./{model_name}_results',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=f'./{model_name}_logs',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_validation_dataset,\n",
    ")\n",
    "\n",
    "trainer.train(f'./{model_name}_results/checkpoint-6000')\n",
    "model.save_pretrained(f'./{model_name}_trained_model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040c1a8d-21de-490c-a8de-a80a790477c6",
   "metadata": {},
   "source": [
    "## 2. Implement LLM models benchmark\n",
    "### a. Fine-tuning T5\n",
    "- T5-large : Trainable. \n",
    "\n",
    "with T5-large, we finetuned t5-large model on total 11668 datapoint for one epoch.\n",
    "```python\n",
    "f\"Predict the percentage of a movie's plot that a character takes up.\\nCharacter: {character_name} \\nPlot: {plot}\"\n",
    "```\n",
    "\n",
    "### b. Instruction tuned-LLM\n",
    "\n",
    "API inference available:\n",
    "- ChatGPT-3.5\n",
    "- ChatGPT-4\n",
    "\n",
    "While chatGPT, force model generation with json format and request estimate the portion of all characters at once. In case of chatgpt failure, when it does not return value for requested field, replace it with linear regression model predicted output.\n",
    "\n",
    "### c. Heuristic baseline\n",
    "- order field in tmdb_credit: the order in credit data of tbdb correlates with his/her importance. \n",
    "- linear regression tuned on order: Fit simple linear regression with order as single feature.\n",
    "- counting character_name on plot\n",
    "\n",
    "Obtained Simple Linear regression models\n",
    "```python\n",
    "def get_portion_by_order_logscale(order):\n",
    "    return np.power(np.e, 0.7756578 - 0.04791 * order)\n",
    "\n",
    "def get_portion_by_order(order):\n",
    "    return max(5.689275 - 0.122673 * order, 0.001)\n",
    "```\n",
    "\n",
    "## 3. Comparison and Model selection\n",
    "For the fare comparison of the model, all model's output would be scaled to make sum of all portions in single plot would be 100.\n",
    "\n",
    "Correlation with gold label, accuracy and f1 score for leading role prediction of threshould 10% will be used as evaluation crietria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "7bc9489d-a58d-42b1-9b34-532b8f92b96e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_set = json.load(open(root_path / 'Preprocessing/plot_portion_dataset.json'))['evaluation']\n",
    "validation_pred = {model_name: defaultdict(dict) for model_name in ['linear','log-linear','tmdb_order','T5-large','ChatGPT-3.5','ChatGPT-4']}\n",
    "tmdb_id2credit = json.load(open(root_path / 'Data/tmdb_resources/tmdb_id2credit_full.json'))\n",
    "target_tmdb_ids = set([v['tmdb_id'] for v in validation_set])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc59193-b09a-4d29-b10c-650c9063258a",
   "metadata": {},
   "source": [
    "### Heuristics, simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "e6436922-a916-46b7-ac79-5dd040fda53e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_portion_by_order_logscale(order): # load as a backup method\n",
    "    return np.power(np.e, 0.7756578 - 0.04791 * order)\n",
    "\n",
    "def get_portion_by_order(order):\n",
    "    return max(5.689275 -0.122673 * order, 0.001)\n",
    "\n",
    "def get_order(tmdb_id, actor_id):\n",
    "    credit = tmdb_id2credit[str(tmdb_id)]\n",
    "    for j in credit['cast']:\n",
    "        if str(j['id']) == str(actor_id):\n",
    "            return j['order']\n",
    "    raise ValueError('invalid actor')\n",
    "\n",
    "def scaling(portions):\n",
    "    total_portions = sum(list(portions.values())) + 0.0001\n",
    "    return {k: 100*v / total_portions for k,v in portions.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "0ef757a2-0926-4777-b65c-cc2b04052799",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for tmdb_id in target_tmdb_ids:\n",
    "    credit = tmdb_id2credit[tmdb_id]\n",
    "    portions_ll = {}\n",
    "    portions_l = {}\n",
    "    portions_o = {}\n",
    "    for c in credit['cast']:\n",
    "        portions_ll[str(c['id'])] = get_portion_by_order_logscale(c['order'])\n",
    "        portions_l[str(c['id'])] = get_portion_by_order(c['order'])\n",
    "        portions_o[str(c['id'])] = c['order']\n",
    "    validation_pred['log-linear'][tmdb_id] = scaling(portions_ll)\n",
    "    validation_pred['linear'][tmdb_id] = scaling(portions_l)\n",
    "    validation_pred['tmdb_order'][tmdb_id] = scaling(portions_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7987b5c-fc15-4011-bdc5-4c73b8cf1712",
   "metadata": {
    "tags": []
   },
   "source": [
    "### GPT-3.5 & GPT-4\n",
    "To compare with \n",
    "```python\n",
    "prompt = f\"\"\"Estimate the percentage of the script that each character represents from the movie plot.\n",
    "[Characters]: {characters_str}\n",
    "[Plot]: {eval_tmdb_id2s[tmdb_id][0]['plot']}\n",
    "\n",
    "Estimate the percentage of the script that each character represents from the movie plot mentioned above. Return the portion of every character in a JSON dictionary, with the character name as key and portion as value.\"\"\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "ce8e72c5-312a-42af-86a6-cb78772b4d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt_pred = json.load(open('chatgpt_pred.json'))\n",
    "chatgpt_pred_4 = json.load(open('chatgpt_4_pred.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "bd58d7bc-d949-4819-a3b1-bc0c4924a4f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hash_string(text):\n",
    "    return text.lower().replace(' ','')\n",
    "def name_match_and_get(tmdb_id,character_id,original_name,generated_dict):\n",
    "    hashed_dict = {hash_string(k):v for k,v in generated_dict.items()}\n",
    "    if hash_string(original_name) in hashed_dict:\n",
    "        try:\n",
    "            return float(str(hashed_dict[hash_string(original_name)]).replace('<','').replace('%','').strip())\n",
    "        except:\n",
    "            return get_portion_by_order_logscale(get_order(tmdb_id,character_id))\n",
    "    return get_portion_by_order_logscale(get_order(tmdb_id,character_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "b8276fb3-87ed-47fa-843f-3ccfb7ccb179",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for tmdb_id,v in chatgpt_pred.items():\n",
    "    output_parsed = json.loads(v['gen_text'])\n",
    "    parsed_prob = {}\n",
    "    for character, c_id in v['character2id'].items():\n",
    "        parsed_prob[c_id] = name_match_and_get(tmdb_id,str(c_id),character,output_parsed)\n",
    "    prob_sum = sum(list(parsed_prob.values()))\n",
    "    validation_pred['ChatGPT-3.5'][tmdb_id] = scaling(parsed_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "a5f1729f-2bff-402f-bbeb-3624389ad7e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for tmdb_id,v in chatgpt_pred_4.items():\n",
    "    try:\n",
    "        output_parsed = json.loads(v['gen_text'])\n",
    "    except:\n",
    "        output_parsed = {}\n",
    "    parsed_prob = {}\n",
    "    for character, c_id in v['character2id'].items():\n",
    "        parsed_prob[c_id] = name_match_and_get(tmdb_id,c_id,character,output_parsed)\n",
    "    validation_pred['ChatGPT-4'][tmdb_id] = scaling(parsed_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c775cb-3ff2-47c8-a7bd-5106f1a2f85a",
   "metadata": {},
   "source": [
    "### T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "a71579de-cc55-4630-8e27-5e246495cdc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t5_output = json.load(open('lead_role_inference/t5_validation_inference.json'))\n",
    "for key, pred in t5_output.items():\n",
    "    tmdb_id, actor_id = key.split('__')\n",
    "    validation_pred['T5-large'][str(tmdb_id)][str(actor_id)]=float(pred['output_text'])\n",
    "for tmdb_id in target_tmdb_ids:\n",
    "    credit = tmdb_id2credit[tmdb_id]\n",
    "    for c in credit['cast']:\n",
    "        if str(c['id']) not in validation_pred['T5-large'][tmdb_id]:\n",
    "            validation_pred['T5-large'][tmdb_id][str(c['id'])] = get_portion_by_order_logscale(c['order'])\n",
    "    validation_pred['T5-large'][tmdb_id] = scaling(validation_pred['T5-large'][tmdb_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "a624eb95-8390-40ed-a531-af62460ccc4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json.dump(validation_pred,open('validation_pred.json','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abefcd68-0566-4267-9f3e-5abc41398f1f",
   "metadata": {},
   "source": [
    "### stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "ea482c3b-cf3a-48a9-894e-65b6cbae3d79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>linear</th>\n",
       "      <th>log-linear</th>\n",
       "      <th>tmdb_order</th>\n",
       "      <th>T5-large</th>\n",
       "      <th>ChatGPT-3.5</th>\n",
       "      <th>ChatGPT-4</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Y</th>\n",
       "      <td>0.372206</td>\n",
       "      <td>0.486409</td>\n",
       "      <td>-0.307976</td>\n",
       "      <td>0.7577</td>\n",
       "      <td>0.777683</td>\n",
       "      <td>0.828316</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     linear  log-linear  tmdb_order  T5-large  ChatGPT-3.5  ChatGPT-4    Y\n",
       "Y  0.372206    0.486409   -0.307976    0.7577     0.777683   0.828316  1.0"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_dict = defaultdict(list)\n",
    "for v in validation_set:\n",
    "    for model, lookup in validation_pred.items():\n",
    "        corr_dict[model].append(lookup[v['tmdb_id']][str(v['character_id'])])\n",
    "    corr_dict['Y'].append(v['portion'])\n",
    "pd.DataFrame(corr_dict).corr()[['Y']].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "300cb406-983d-4b37-9c53-5e77cf5e25ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear      \tAcc:0.80429\tF1:0.11429\tAUC:0.52219\n",
      "log-linear  \tAcc:0.80934\tF1:0.14689\tAUC:0.53291\n",
      "T5-large    \tAcc:0.88510\tF1:0.68070\tAUC:0.79233\n",
      "ChatGPT-3.5 \tAcc:0.90657\tF1:0.72593\tAUC:0.80812\n",
      "ChatGPT-4   \tAcc:0.90152\tF1:0.73103\tAUC:0.82525\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "result_dict = corr_dict.copy()\n",
    "def classify_scores(scores):\n",
    "    return [1 if score > 10 else 0 for score in scores]\n",
    "\n",
    "for k,v in result_dict.items():\n",
    "    result_dict[k] = classify_scores(v)\n",
    "\n",
    "for model_name, model_predictions in result_dict.items():\n",
    "    if model_name not in  ['Y','tmdb_order']:\n",
    "        print(f\"{model_name:<12}\\tAcc:{accuracy_score(result_dict['Y'], model_predictions):.5f}\\tF1:{f1_score(result_dict['Y'], model_predictions):.5f}\\tAUC:{roc_auc_score(y_true_classified, model_predictions):.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc96b0bf-8a8f-428a-9a91-7d2cd3238468",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
